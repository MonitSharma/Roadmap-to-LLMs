# Learning Roadmap for Large Language Models (LLMs)

## Topics and Progress Tracking

- [ ] **Introduction to Large Language Models (LLMs)**
  - Description: Gain a foundational understanding of what LLMs are, their significance, and their applications.
  - Resource: [Introduction to Large Language Models](https://developers.google.com/machine-learning/resources/intro-llms)

- [ ] **Transformer Architecture**
  - Description: Study the core architecture behind many LLMs, focusing on components like self-attention and positional encoding.
  - Resource: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- [ ] **Self-Attention Mechanism**
  - Description: Delve into how models weigh the importance of different words in a sequence to capture context.
  - Resource: [Understanding and Coding the Self-Attention Mechanism of Large Language Models](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)

- [ ] **Training Large Language Models**
  - Description: Learn about the processes involved in training LLMs, including data collection, preprocessing, and computational requirements.
  - Resource: [From bare metal to a 70B model: infrastructure set-up and scripts](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles)

- [ ] **Reinforcement Learning in LLMs**
  - Description: Explore how reinforcement learning is applied to enhance LLMs, focusing on models like DeepSeek-R1.
  - Resource: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

- [ ] **Improving LLM Performance Without Increasing Parameters**
  - Description: Investigate techniques to enhance model performance efficiently, such as knowledge distillation and quantization.
  - Resource: [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)

- [ ] **Advanced Architectures: Mixture of Experts (MoE)**
  - Description: Understand how MoE architectures can improve model efficiency by activating only relevant parts during inference.
  - Resource: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

- [ ] **Practical Implementation and Experimentation**
  - Description: Gain hands-on experience by implementing components like the self-attention mechanism from scratch.
  - Resource: [Understanding and Coding the Self-Attention Mechanism of Large Language Models](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)

- [ ] **Staying Updated with Recent Developments**
  - Description: Keep abreast of the latest research, models, and techniques in the rapidly evolving field of LLMs.
  - Resource: [Awesome-LLM: A Curated List of Large Language Models](https://github.com/Hannibal046/Awesome-LLM)
